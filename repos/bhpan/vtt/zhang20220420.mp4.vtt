WEBVTT

0
00:00:00.340 --> 00:00:15.620
说的序列标注任务的头哈 那么这次课一会儿我们会讲序列标注任务 因为现在很多序列标注任务 包括我们的双向LSTM等等

1
00:00:16.150 --> 00:00:23.710
方法里面有的时候也需要用到注意力机制 包括我们的transformer 包括我们的BERT Model

2
00:00:24.510 --> 00:00:35.230
以及现在很多的其他的预训练的模型里面 包括我们在自己构造自己的模型的过程当中 都会用到注意力机制

3
00:00:35.910 --> 00:00:43.550
这样的方式或者是方法 那么这次课呢 我们就看一下注意力机制 实际上注意力机制

4
00:00:44.090 --> 00:00:51.290
这一块呢确实给我们提供了很好的解释

5
00:00:51.890 --> 00:00:59.690
神经网络的方法 那么一块呢 还是比较比较 重要的内容

6
00:01:01.150 --> 00:01:06.790
但注意力机制呢相对来讲比较简单 也很实用

7
00:01:07.490 --> 00:01:14.730
那么  在 尤其是现在哈 那么有各种各样的尝试

8
00:01:15.640 --> 00:01:21.040
通过结合注意力机制 那么来提升模型的效果性能

9
00:01:21.610 --> 00:01:26.690
很简单 想法也很简单 那么

10
00:01:29.680 --> 00:01:34.520
我们这次课呢就来看一下哈就注意力机制的基本的方法

11
00:01:35.520 --> 00:01:46.920
和他的思想以及呢 怎么样去使用 相信的有很多同学也都在工作的过程当中用到过 相关的些模型

12
00:01:47.470 --> 00:01:56.350
技术方法等等 那么谈到注意力的时候呢 我们肯定 就会

13
00:01:57.150 --> 00:02:09.390
首先介绍一下叫做注意力机制注意力机制呢在里面呢一关我们是吧分神东西是吧

14
00:02:12.080 --> 00:02:17.280
真是attention的比较常用比较常用做法

15
00:02:17.770 --> 00:02:24.850
那么他呢 是从哪里 被借鉴来的呢 一般来讲 我们认为是从

16
00:02:25.420 --> 00:02:30.900
神经科学里面的注意力的思想 接下来

17
00:02:32.080 --> 00:02:37.560
本身呢 我们也常说神经网络 尤其是地人拧过地铺猫的

18
00:02:38.430 --> 00:02:45.710
都是基于 参考神经学的一些内容 还有脑科学的一些内容 那我们看一下

19
00:02:46.390 --> 00:02:53.950
在神经 或或者叫做认知神经学当中 注意力 他是这样的

20
00:02:54.690 --> 00:03:00.010
想法 那么attention呢 他指的是 据说我

21
00:03:00.810 --> 00:03:06.690
我们的人脑哈 尤其是视觉神经一块哈

22
00:03:07.270 --> 00:03:12.590
那么他的每个时刻接受的外界的输入的学习都非常多

23
00:03:14.020 --> 00:03:20.860
那么包括视觉呀听觉呀嗅觉呀触觉呀等等各种各样的信息

24
00:03:21.490 --> 00:03:29.770
都有 那么对于视觉来说 你会发现实际上我们眼睛看到的东西都是高清的

25
00:03:30.380 --> 00:03:46.420
那么样大量的信息实际上他传递 如果单独把些信息不经过压缩 不经过编码的方式直接传输给大脑的话 每秒钟会需要千万个比特的种信息

26
00:03:47.810 --> 00:03:54.810
 但是呢 有一些部分的功能 它是被我们的视觉神经系统给处理了

27
00:03:55.690 --> 00:04:03.650
就视觉神经系统 它传给我们人脑的经过压缩过过后的数据

28
00:04:04.700 --> 00:04:11.420
那么实际上视觉神经系统是会发达的哈但是人眼睛我觉得那眼睛构造

29
00:04:12.250 --> 00:04:19.610
换你包括玻璃体压等等 他还是很很很微妙的 包括我们的师生经

30
00:04:20.790 --> 00:04:27.550
是神经也是基本上是不可逆的 哈 很重要 那么是神经呢 他就

31
00:04:28.070 --> 00:04:37.510
他有压缩功能 他传递给他了 那么我们的大脑呢 实际上他会有注意力机制

32
00:04:38.880 --> 00:04:46.040
视觉神经虽然以很快的速度那么将它捕捉到了些

33
00:04:46.700 --> 00:04:53.500
 环境的 些视觉的信号传送给大脑之后呢 大脑是通过注意力机制

34
00:04:54.250 --> 00:04:59.850
来解决信息超载的问题 数据量太多了 那么注意力机制体现在哪呢

35
00:05:00.350 --> 00:05:12.750
主要就体现在我们可以看到了是吧我们注意了不同的地方 比如说我们看文章的时候 你看在信息检索里面 或者在心理学里面 大家经常会看到图形

36
00:05:13.710 --> 00:05:21.310
就说我们人的眼睛会聚焦到哪个位置 尤其是在信息检索里面 大家经常会用到

37
00:05:21.980 --> 00:05:35.060
 做一些实验 所谓的演动实验那么的演动实验一方面的是看你或环境的变化 你的眼睛眼动 有没有病 是吧 包括可以治愈一些抑郁症 

38
00:05:35.670 --> 00:05:41.550
另外一方面 他可以看眼睛的福克斯到的位置上 那呢我们在

39
00:05:42.460 --> 00:05:49.900
人人阅读的过程当中哈 那你看是一篇文章 他的题目是吧 比较受读者的关注

40
00:05:50.360 --> 00:05:56.960
再摘要的部分 如果看人的话 那我们是不是先看一下脸 是吧 那话呢

41
00:05:57.360 --> 00:06:04.800
还有部分的些四肢的些结构哈 那么会重点的关注

42
00:06:05.510 --> 00:06:13.470
呢 注意力 实际上 我们注意力机制 他不光是在视觉里面有研究

43
00:06:14.360 --> 00:06:20.160
还有呢 比如说在认知的过程当中 我们的听觉也是一样

44
00:06:20.860 --> 00:06:29.740
里面的有鸡尾酒效应 在酒会上 朋友聊天 虽然周围的噪音的感染很多 但是

45
00:06:30.270 --> 00:06:38.510
他的 因为同时会输入到很多的声音 但是呢 他还是可以跟朋友进行交谈

46
00:06:38.950 --> 00:06:45.710
在展 叫做聚焦式的注意力 也他可以忽略到背景声当中

47
00:06:46.610 --> 00:06:51.970
嘈杂的声音 但是如果背景声当中

48
00:06:52.940 --> 00:07:01.060
有重要的词汇 它会很快的引起它的主意 那么时候呢 如果背景声当中出现一些重要的词

49
00:07:01.830 --> 00:07:07.630
那么就可以 几乎 显著性的 注意力的机制

50
00:07:09.220 --> 00:07:14.540
那么 在里面呢 刚才提到了两个特点 是聚焦式注意力 也是显著性注意

51
00:07:15.080 --> 00:07:22.520
聚焦是注意义的 他是有预定目的的 依赖于任务的人 需要有主动的意识去聚焦于

52
00:07:22.960 --> 00:07:35.960
对象的注意 那两个人进行交谈 实际上 我们在在我们的sprark那一生的时候也有很多研究 是来提升 在嘈杂环境下 那么

53
00:07:36.780 --> 00:07:43.980
声音收音的种种识别的种效果显著性注意力呢 主要是由外界的种驱动

54
00:07:44.280 --> 00:07:51.440
 不需要主动的干预 也通用 他和任务是没有关系的

55
00:07:52.240 --> 00:07:58.600
一般来讲呢 我们认为会出现两种数据

56
00:07:59.270 --> 00:08:06.230
数据注意力的类型 那我们看一下 我们在神经网络里面的注意力 到底是如何

57
00:08:07.380 --> 00:08:12.940
实现的或者他基本是原理是样的 我们呢 

58
00:08:14.980 --> 00:08:23.060
首先看哈我们为要需要注意力机制哈就注意力机制是因为我们要处理或者是我们要

59
00:08:23.990 --> 00:08:29.230
记忆的信息比较多 那话呢 实际上

60
00:08:29.800 --> 00:08:38.880
 我们说从之前在我们讲瑞科润t牛肉never的时候循环摄影网络的时候 我们就

61
00:08:39.850 --> 00:08:47.450
提到过是吧有证明 它呢是可以模拟 日益形式的函数实际上mp

62
00:08:48.080 --> 00:08:55.000
我们经常用的多层感知机m o p样

63
00:08:55.500 --> 00:09:07.420
实际上 它很简单 当它曾达到一定程度的时候 也被证明了 它可以模拟任意任意形式的函数 那么函数模拟的话 它是需要大量的

64
00:09:08.200 --> 00:09:19.600
复杂的种网络的连接结构 才能够让他有种效果 当然我我们说没有办法 我们没有办法创造出足够

65
00:09:20.480 --> 00:09:27.320
大的模型来去模拟任意任何函数 而且我们的训练数据往往是有噪音的 我们也达不到

66
00:09:27.800 --> 00:09:40.440
 而且呢 我们知道机器学习呢 它依赖于大规模的种标注数据 标注数据里面有噪音的话 也会有模型的性能产生影响 说 受限于

67
00:09:41.050 --> 00:09:49.010
些问题 我们 需要 从记忆到的些信息里面

68
00:09:49.590 --> 00:09:55.350
或者是我们输入给模型或者结构模型变化之后的些信息里面

69
00:09:55.830 --> 00:10:04.150
去选择有用的信息 那么告诉模型应该注意到哪些位置

70
00:10:04.680 --> 00:10:12.840
哪些有用的信息 在里面信息呢 我们大多数的情况下是指神经网络 是英元的输出

71
00:10:13.350 --> 00:10:24.430
 神经网络的视野是宁远的 输出信息只能是经过处理 上涨

72
00:10:29.530 --> 00:10:33.050
神经不行

73
00:10:35.850 --> 00:10:40.170
我们所谓的信息 指的就呢

74
00:10:44.040 --> 00:10:47.880
当模型变得越来越复杂的时候 我们

75
00:10:49.170 --> 00:10:54.770
需要助理更多的种信息 那么

76
00:10:55.430 --> 00:11:04.310
我们就提出用记忆力种方式 同时呢实验记忆力还让我们提供了一种权重的机制

77
00:11:05.600 --> 00:11:10.720
 那么权重机制呢 可以去衡量到底神经网络

78
00:11:12.480 --> 00:11:17.560
在计算的过程当中 哪部分起的作用最大 

79
00:11:18.630 --> 00:11:23.750
 是我们需要注意力的原因 注意及时 原因哈 那么呢

80
00:11:24.860 --> 00:11:31.460
我们通常用到的一种注意力

81
00:11:31.860 --> 00:11:46.860
机智的性 在里面的 我们以rn网络来举例 是双层的rn的模型 你们看到的

82
00:11:47.680 --> 00:11:53.920
是 叫做翻译 从中文翻译成法语

83
00:11:56.730 --> 00:12:00.810
很爱吗 student翻译成法语

84
00:12:03.290 --> 00:12:08.770
那么 翻译成法语的时候呢 实际上 如果传统的话 他是没有

85
00:12:09.510 --> 00:12:15.950
灰颜色框里面部分的话他没有注意力机制的 话呢 所谓的

86
00:12:16.510 --> 00:12:24.470
压缩压缩的时候 他呢 是把所有的信息都统一的压缩到最后单元

87
00:12:25.840 --> 00:12:32.760
压缩到儿 在地方的输入给解码器 那chstocksmall的基本的

88
00:12:33.350 --> 00:12:43.030
形态 再让他去解码 但是呢 我们认为哈在我们前面进行压缩的过程当中

89
00:12:43.670 --> 00:12:50.110
 就说 当我进行翻译的时候 不是每词对他的贡献都是一致的

90
00:12:51.970 --> 00:12:58.930
 在我们进行解码的时候 是吧词是他对应的是爱 位置比较重要一些

91
00:12:59.890 --> 00:13:08.490
词算词 词对应爱的位置是解码器 输出在上面词那是

92
00:13:09.430 --> 00:13:14.870
或者是最后id店词呢是对应的是

93
00:13:15.760 --> 00:13:21.800
 在解码的过程当中 我们会发现

94
00:13:22.340 --> 00:13:27.820
每词 他从原来原始的输输出

95
00:13:28.570 --> 00:13:41.490
或者压缩的结果前面的不同的词给他的贡献都是不一样 因此呢 我们想要用注意力机制的 在我去进行解码的过程当中 我

96
00:13:42.140 --> 00:13:51.700
会有权重来限制前面序列 他的计算的结果为当前时刻带来的影响

97
00:13:53.170 --> 00:13:59.850
不然的话大家都是均等的是吧 说呢 在里面呢 引入了

98
00:14:01.910 --> 00:14:08.030
注意力机制 他的通过权重大家要注意的是他们的权重加起来

99
00:14:08.560 --> 00:14:16.040
等于一也说呢 可以 那是意思

100
00:14:18.120 --> 00:14:23.200
就说呢 些权重加起来是加全球和等于一

101
00:14:24.210 --> 00:14:32.970
加钱求合的 呢他大家注意区分另外一件事哈 可以大家的客户去思考

102
00:14:33.810 --> 00:14:41.770
我们呢想一想哈就注意力机制给给挺机制大家之前我们在

103
00:14:42.470 --> 00:14:50.670
rn的时候讲过哈他里面有不同的门哈我们的attention

104
00:14:54.450 --> 00:14:59.250
和跟平 两个都是两个不同的机制

105
00:14:59.940 --> 00:15:05.900
希望大家能够思考一下 就两个机制 他们都不同意 他们都不同是

106
00:15:06.950 --> 00:15:12.710
大家可以思考 attation的是要求哈每

107
00:15:13.170 --> 00:15:21.610
 位置 假设有四个位置是吧 他们的权重核是一而该艇机制是在每位置上

108
00:15:22.340 --> 00:15:28.420
他输出和他不输出也他开或者是关两个概率

109
00:15:28.810 --> 00:15:37.810
但是一但是整体上来 他求和不是一呢 

110
00:15:39.750 --> 00:15:48.430
大家会看到哈 他实际上和我们之前说过的m p哈全连接的网络哈

111
00:15:49.100 --> 00:15:56.740
有类似之处有类似之处类似之处他的 但是也有不同 不同其实挺明显的

112
00:15:56.950 --> 00:16:05.030
我们m p哈它的输入是 说相同的 对吧 输入给m p 它长度是一样的

113
00:16:05.580 --> 00:16:12.300
但是呢 attation它有好处 好处呢 它是可以解决变长的问题 就在里面

114
00:16:12.760 --> 00:16:20.120
我想要控制假设12344个额输入 作为快位 输入的时候

115
00:16:20.650 --> 00:16:27.650
那么额 它可以输出四个天使 如果要是五个 要是六个

116
00:16:28.100 --> 00:16:35.460
他也可以去对五个或者六个来进行权证的筛选 那是他的好处

117
00:16:40.090 --> 00:16:43.730
实际上 了 他的实现的思想和我们的给点机制是类似

118
00:16:45.050 --> 00:16:50.490
妈妈 我们看一下哈绿地提示 他的

119
00:16:51.350 --> 00:16:58.310
计算流程 计算流程我们认为基于注意力机制 它呢主要分为三步

120
00:17:00.680 --> 00:17:07.840
在里面的有q叫做cry 有输入项链是x注意的

121
00:17:08.580 --> 00:17:15.460
是给定的 给定的宽瑞 在对应公公司里面 就宽瑞是q

122
00:17:16.330 --> 00:17:21.970
输入项链x 两个都是给定 我们来计算

123
00:17:22.540 --> 00:17:30.140
每个输入项链和项链输入项链和查询项链之间的

124
00:17:30.530 --> 00:17:43.210
相关性 注意里面计算的是 大家要看清楚哈 是输入销量 输入下来x

125
00:17:44.030 --> 00:17:50.790
x是项链和查询一下两q q也是项链两个词之间的

126
00:17:51.340 --> 00:17:56.660
相关性 那么华瑞是查询限量x

127
00:17:57.470 --> 00:18:04.310
输入下来呢 大家看到公式里面还有z z呢 是表示

128
00:18:04.980 --> 00:18:11.860
位置 或者是信息 他被选择的 的位置 每位置

129
00:18:12.640 --> 00:18:18.240
比如说在里面限量里面比如说

130
00:18:19.370 --> 00:18:24.730
输入了n的响量 一共输入了n的响量 那么q

131
00:18:25.710 --> 00:18:30.990
和x之间的相关性就要从大x里面选择

132
00:18:31.480 --> 00:18:41.800
到底是每销量和q的相关性都有多少 说爱相当于是index 你输入的

133
00:18:42.180 --> 00:18:58.900
向量若干个向量 它的硬带是多少 那么时候呢 它是输入 向量和另外 输入向量 查询限量之间查询限量是不变的 输入限量是变的 对吧 那么在里面呢

134
00:18:59.350 --> 00:19:07.750
实际上需要用概率函数来模拟 因为里面求的是有probability概率函数可以用softnight函数

135
00:19:08.410 --> 00:19:14.370
时候我们就要上次课 上节课我们讲到小麦的函数 你会看到骚麦的函数非常的重要

136
00:19:15.640 --> 00:19:21.320
那通过softmax函数来求解x i和q

137
00:19:21.970 --> 00:19:32.930
那他的下速度 当然种计算方式只是其中的一种一种 那么我们注意到s 它对应的是的 对应的是错方式

138
00:19:33.480 --> 00:19:41.320
打分函数 打分函数的我们没有明确的告诉大家打分函数是

139
00:19:41.640 --> 00:19:50.600
 一般来讲呢 我们在第二步的时候  就需要为注意力打分机制提供计算函数

140
00:19:51.900 --> 00:19:56.780
 那么打分机制主要包括加性模型

141
00:19:57.100 --> 00:20:02.460
 引入碳函数 引入碳函数 那么

142
00:20:03.250 --> 00:20:08.530
会对x i有变化再加上uq有变化

143
00:20:10.160 --> 00:20:18.960
那么 实际上我们看都是线性变化 是吧 线性变化对吧 通过现象函数对x i进行变化

144
00:20:19.660 --> 00:20:29.140
呢 再对额u 再对qcry 再进行 谢谢美化 求和 再调t h

145
00:20:30.780 --> 00:20:36.020
有的同学说的对 是不是similarity 是

146
00:20:36.660 --> 00:20:43.300
第二个叫做点击模型 之前我们也提到哈 提到过  点击模型

147
00:20:44.200 --> 00:20:48.520
就可以等同于下十度是吧 等同于下十度是有那样

148
00:20:50.400 --> 00:20:57.480
但是呢 我们在点点点击模型的基础上 我们有的时候会用到双线性模型

149
00:20:58.120 --> 00:21:07.800
去计算思维了 那么是原理呢 假设哈 假设假设呀 我们有矩阵哈有矩阵 矩阵是

150
00:21:08.410 --> 00:21:15.410
呢 我们还会有 在里面呢 我们会有

151
00:21:17.450 --> 00:21:26.250
里面我用不了笔了 假设它的对角线 假设了它有对角线 对角线是 对角线上有一些元素

152
00:21:29.890 --> 00:21:34.570
对角线上有一些元素哈 那么对角线上的元素呢

153
00:21:35.610 --> 00:21:40.930
 假设他是对角证 哈假设是对角证 那么

154
00:21:42.060 --> 00:21:51.900
 哎 为啥起床22

155
00:21:54.320 --> 00:21:59.000
对角线上是 其他的位置

156
00:21:59.620 --> 00:22:05.980
都是零 假设最简单的双线性模型最简单的双线性模型 其他位置

157
00:22:06.460 --> 00:22:12.940
都按零处理 都按零处理 就地儿是零 地儿

158
00:22:14.280 --> 00:22:26.520
也是零 在

159
00:22:27.520 --> 00:22:37.080
我只有对角线上有数字元素 那么简化了的双线性 过去大家可以看一下

160
00:22:37.800 --> 00:22:49.040
如果对角只有 假设是x i和q进行双线性操作的时候 那么周只有中间 位置有数的时候

161
00:22:49.930 --> 00:22:55.010
表示呀 实际上表示x i transpose转制

162
00:22:55.470 --> 00:23:04.790
乘以q的对应位置 它是对应位置 相乘对应位置 相乘之后中间还会有

163
00:23:05.490 --> 00:23:11.170
w 也成的时候 对于x i的其中的位置

164
00:23:11.640 --> 00:23:17.360
 比如说位置是k dk个位置 对于dk的位置 我在计算的时候

165
00:23:17.700 --> 00:23:25.580
 比如说 在里面 我们用到了dk行 当然他肯定是dk行 dk列

166
00:23:27.830 --> 00:23:37.350
dk行位置的时候 那么他肯定是对于x i向量的d k个位置和q向量的dk个位置 他们进行

167
00:23:38.040 --> 00:23:47.280
wicked的成绩 也经过了乘上了位置所对于权重

168
00:23:48.190 --> 00:23:54.590
之后的点程 之后点程 说

169
00:23:55.080 --> 00:24:00.920
我们看到双线性模型 它呢对于x i和q

170
00:24:02.040 --> 00:24:08.520
两个项链的不同维度上 他们会有权重的控制 说在里面 我们

171
00:24:09.010 --> 00:24:18.290
x i t乘以中间的转换的矩阵w 再乘以个q 种双线性模型是比较常用的一种形式

172
00:24:19.400 --> 00:24:27.280
当然 w是可以学出来的 有注意 w是神经网络 它的参数是学出来的 那么我们可以把它简化成种形式

173
00:24:27.570 --> 00:24:38.170
大家就可以理解 他究竟是在干 他实际上还是在算相似度 只不过相似度他是经过了内部计算的时候 对每维度加圈

174
00:24:38.870 --> 00:24:50.110
 话可以让你的相似度的模型学得更灵活 仅此而已 说有的时候大家会会在神经网络模型里面看论文的或者看一些介绍的时候会引入

175
00:24:51.760 --> 00:25:02.360
双线性的情况 另外呢 缩放点击模型 就说你乘出来数话 需需要经过修正 那么呢 我们可以有一些

176
00:25:03.170 --> 00:25:10.690
比如说数我不让他太大 对吧 不让他太大因为你s i和q你都没有进行种normalize都没有规划

177
00:25:11.700 --> 00:25:22.140
成熟说有太大 那我要拿出来之后他还太小 对我的模型的还帮助还不大 对吧 或者是训练会变慢 种情况下的我们可以用缩放点击的模型

178
00:25:23.570 --> 00:25:31.130
种模型 种模型 说呢 你会看到第二部 我们会选择一种打分函数

179
00:25:31.430 --> 00:25:42.670
 说而且哈 我们在现在的很多的时间网络里面哈 你骚麦克函数里面 你会求方式 那么方式你可以指定

180
00:25:43.320 --> 00:25:56.120
形式 制定样形式  稍卖财数是吧东西上次咱们讲过对吧是就相当于

181
00:25:56.640 --> 00:26:06.000
当前位置的school当前位置错除以所有的输入它所对应的school的清河的形式

182
00:26:06.290 --> 00:26:11.890
 那么当然阿f i算完了之后阿f一阿f i

183
00:26:12.370 --> 00:26:20.450
二意识到 阿富 他们求和对 说 回到前面位置 你就会看到地方是

184
00:26:20.910 --> 00:26:26.430
x一哈是x二x三x

185
00:26:27.950 --> 00:26:33.750
输入对应不同的输入 最后他对应每位置 他把注意力输入

186
00:26:34.480 --> 00:26:39.920
进行计算货方式得分分之后都会有最终的

187
00:26:41.040 --> 00:26:51.760
阿法的纸 那么阿法的纸是根据打分的 有些同学说东西为注意力机制就么公式 它为它就好用呢

188
00:26:54.020 --> 00:26:59.700
 他权重的机制嘛 说来说去对吧

189
00:27:00.280 --> 00:27:09.080
确实是 就么么简单 方式 为他会起作用 因为 你会看到 无论是家性模型

190
00:27:09.650 --> 00:27:17.290
还是点击模型还是双线性模型 在里面的都会有参数进行变化

191
00:27:18.560 --> 00:27:23.720
 比如说我们在最简单的 比如说我们的点击

192
00:27:24.350 --> 00:27:31.990
点击里面 实际上在在模型里面 s i 它呢是随着训练

193
00:27:32.930 --> 00:27:42.010
过程会不断的更新的 包括我们的稍微复杂一点的双线性模型 或者是刚才我举的例子

194
00:27:42.550 --> 00:27:48.430
 对角线上 其他位置都是零只有对角线上非零的 方法

195
00:27:48.790 --> 00:27:58.230
他 实际上w也是跟着模型学的 说我们期待的是注意力机制里面用到的些参数是随着训练的过程

196
00:27:58.670 --> 00:28:05.270
会被调整 那么调整到最最后好的形式说能够把

197
00:28:05.720 --> 00:28:10.880
注意力机制学好 让他对于每不同的宽容

198
00:28:12.050 --> 00:28:19.850
都会有不同的处处 那么

199
00:28:20.550 --> 00:28:30.470
实际上哈就可以举实际的例子哈实际例子比如说 我有样需求哈比如说在在外面处理里面

200
00:28:31.030 --> 00:28:36.750
我 我进行简单的工作哈 我有样例子哈 哎的

201
00:28:37.940 --> 00:28:41.900
是对大的

202
00:28:44.720 --> 00:28:51.440
thirsthisby假设针对于样一句话

203
00:28:51.850 --> 00:28:58.170
我们的  想要分析一下哈 他

204
00:28:59.130 --> 00:29:08.890
他的情感到底是 对吧情感到底是 但是呢 我们会发现的句话里面对吧 实际上呢

205
00:29:09.380 --> 00:29:16.420
是存在了两个不同的 叫评论的 

206
00:29:17.280 --> 00:29:24.840
维度 是负 是service 那么种情况下我要直接把句子

207
00:29:25.390 --> 00:29:32.390
输入到我的神经网络 无论是r n c或者是倍儿模型的时候 他说出 你问他

208
00:29:33.670 --> 00:29:43.870
句话表达了情感 你你说不太出来的吧 他说到有好有坏 对吧 你必须要有方面假设任务变成

209
00:29:45.070 --> 00:29:54.070
额句话里面都评论了就他评论对象对应的情况分别是 那你就可以把负的作为

210
00:29:54.420 --> 00:30:00.380
范瑞 输入到模型里面 而

211
00:30:01.370 --> 00:30:10.730
复的输出输入的时候 他就会把attention 让attention机制聚焦到good的词上

212
00:30:11.490 --> 00:30:17.410
 就要到够的词上样财神机制他就work

213
00:30:17.960 --> 00:30:24.280
对吧 根据输入 如果你的输入是service 它就会让时间网络更加的聚焦到

214
00:30:24.700 --> 00:30:30.300
败的位置上 败的位置上 呢 你就会发现

215
00:30:31.080 --> 00:30:38.120
我们注意力机制样起作用 你可以根据你的花味的不同 food还是siri

216
00:30:39.260 --> 00:30:45.020
来让模型更focus到哪位置上哪位置

217
00:30:47.160 --> 00:30:51.640
简单的举举了几个例子哈说注意力机制是想实现

218
00:30:52.190 --> 00:30:59.430
说在很多的应用里面 我们实际上都想要达到种效果

219
00:31:00.560 --> 00:31:07.040
说呢种方式 有的时候我们也叫寓意的注意力

220
00:31:07.660 --> 00:31:14.260
 就他想要让你的不同位置上的输出在处理不同的信息的时候

221
00:31:14.730 --> 00:31:20.770
它可以输出特定的 信息

222
00:31:23.320 --> 00:31:29.720
ok 我们再来看他的第三步 刚才说的是方式 就怎么样去进行计算

223
00:31:32.230 --> 00:31:35.910
第三步呢 我们要进行加群 平均

224
00:31:37.390 --> 00:31:52.150
那么加权平均呢 实际上也很简单对吧 说我们可以根据刚才的注意力机制算出来了 阿凡11阿法21阿凡

225
00:31:54.140 --> 00:32:00.660
那么我快人来了之后 我对每位置都有

226
00:32:04.460 --> 00:32:17.060
但是呢 不是说只有最大的阿法可以阿法艾所对应的位置的x x 针对于attention 输入起作用实际上大家都有作用

227
00:32:17.670 --> 00:32:23.270
只不过作用有大有小而已 传统来讲我们就只是取了平均 是吧

228
00:32:23.860 --> 00:32:35.100
当然 我之前说过如果我们用贝尔模型的话 平均效果就不错了 但是我们想进一步提升它的效果的时候 我们可以用 f 1x以x一

229
00:32:35.550 --> 00:32:41.110
到里面 注意哈 是阿法一里面有操作是乘法

230
00:32:41.720 --> 00:32:47.760
承受你的输入 到位置来 对吧

231
00:32:48.390 --> 00:32:55.230
f 2x以输入x二给到位置 到f n乘以x n

232
00:32:55.600 --> 00:33:02.320
 给到位置了 那么话 在最后的位置上有综合

233
00:33:02.720 --> 00:33:09.000
甲醛平局 根据我们计算出来的attation的valuef one到

234
00:33:10.590 --> 00:33:15.390
我们进行了加权的平均 就公式

235
00:33:17.170 --> 00:33:26.130
那么 也说 对于查询q 那么它的不同的输入向量都会有

236
00:33:26.560 --> 00:33:44.880
一定程度的被关注度 那么我们在里面就有一种软性的信息选择机制对输入信息汇总 你看在他会汇总信息 说 我们可以看到同样的x 1-0x的序列 当它的宽锐不同的时候

237
00:33:45.480 --> 00:33:52.840
我们得到的最终的 处理效果也是不一样的 说在里面我们要只能注意的

238
00:33:53.420 --> 00:34:00.740
的哎呀很难操作东西哈叫做软性机制

239
00:34:07.170 --> 00:34:15.650
soft经常我们说要soft猫的 soft对应的不是号人哈

240
00:34:16.300 --> 00:34:24.460
soft实际上是和哈尔的对应哈二代的话题要么零样的一 soft他是零点几

241
00:34:28.720 --> 00:34:40.240
说呢 刚才有同学问是吧 阿法 他实际上对应的是 不是a 是阿法哈对应的是他的权重 也他起重要程度的

242
00:34:40.730 --> 00:34:50.530
比例 那么话 我们就可以看到整体的注意力机制 它的计算流程 你会看到

243
00:34:53.370 --> 00:34:57.690
从我们的解码的时候哈 我们会考虑到attation之后的

244
00:34:58.430 --> 00:35:04.350
factor会被解码的过程所考虑 包括问题也是一样

245
00:35:04.800 --> 00:35:16.400
 样问题的呀 我编码的过程当中 最后输出结果 比如说我用bur做cs的时候 如果我把foot做tation的时候

246
00:35:16.710 --> 00:35:33.910
我们也可以 进行加权的 种平均最后刚才呢 实际上我们有进行扩展哈 要进行扩展

247
00:35:34.230 --> 00:35:41.230
 刚才我们说的了是一种推升的一种简单的一种

248
00:35:42.010 --> 00:35:46.850
方式实际上呢 attention呢 他有 他被总结成

249
00:35:48.880 --> 00:35:58.240
prary的value种形式 那么在注意力机制里面 最开始提出 大家都是一种简单的模型

250
00:35:58.920 --> 00:36:04.840
简单模型 它实际上呢可以被总结成种

251
00:36:05.570 --> 00:36:11.930
兼职 也kv六attation种形式 

252
00:36:12.390 --> 00:36:18.070
k白领 大家可以看到的是v

253
00:36:18.980 --> 00:36:23.900
 他是在外面的哈在外面的我们有不同的

254
00:36:25.080 --> 00:36:33.000
 在里面 k相当于是作为输入 每k实际上对应了其中的

255
00:36:33.440 --> 00:36:39.760
v6 0如说k一对应的v k 二对应是b 二意识到kn对应的是b n

256
00:36:40.610 --> 00:36:49.730
那么宽瑞在计算的时候 实际上宽瑞适合value 你看到的val 他所对应的

257
00:36:51.010 --> 00:36:55.930
来进行的操作 说呢  

258
00:36:56.930 --> 00:37:04.210
注意力 就刚才我们说的注意力机制可以泛化到kv六模型工作的形式

259
00:37:05.100 --> 00:37:11.820
那么时候呢 我们f i乘以v 1x以v21直乘以到v n

260
00:37:12.500 --> 00:37:18.100
他们做加全球合的时候 那么阿法的产生或者阿法计算是怎么算的呢

261
00:37:18.640 --> 00:37:27.320
是根据k和v六他所对应的形式算出来的 思考方式还是和原来一样的

262
00:37:27.650 --> 00:37:34.130
 我们看到了很多的工作里面是把k和v六

263
00:37:34.690 --> 00:37:42.290
 作为相同的输入 作为相同一说一会儿我们会看到对应的

264
00:37:45.470 --> 00:37:47.990
你相当说 三分开始 我们有吧

265
00:37:52.870 --> 00:38:00.830
那么当还有其他的变种还有其他

266
00:38:02.400 --> 00:38:11.400
变种的当查询q 宽瑞 它只和输入系列当中的某元素相关 也就比如说 我可以选最大的

267
00:38:15.660 --> 00:38:18.700
那么时候叫做硬注意力机制

268
00:38:19.730 --> 00:38:27.930
 只关注某位置上的信息 就我把noise都去掉了 我认为只有最重要的 我们才需要关注 也有样做

269
00:38:28.450 --> 00:38:38.170
 那么呢 硬注意力 硬注意力 它只输出最高的概率所对应的输入

270
00:38:39.090 --> 00:38:41.810
x 在里面是x j

271
00:38:48.650 --> 00:38:52.730
有的时候呢 我们的硬注意力 可以通过注意力分布

272
00:38:53.540 --> 00:39:03.140
 没有事注意力分布 比如说注意力 我们可以计算出阿法11直到阿法样分布 我们可以根据分布进行怎样

273
00:39:03.890 --> 00:39:12.730
 进行采用 当然你概率最大的 他可以被采到的情况最大 是吧最多对吧

274
00:39:13.760 --> 00:39:24.440
是在分布上进行材料 随机材料 对吧 你分布如果某位置打法值高的话 它被采购的概率当然会高

275
00:39:25.950 --> 00:39:31.990
应注意力机制 话可以过滤到一些噪音 但是呢 它有缺点

276
00:39:32.660 --> 00:39:46.820
 缺点就说呢 种形式的不可导对吧 因为它是节约函数了 没有办法用反向传播算法进行训练 在里面呢 需要指说题外话就说不是说我们所有的

277
00:39:47.340 --> 00:39:56.100
神经网络都需要用反向传播算法进行训练的 反向bbbgation只是对我们训练神经网络方法当中的一种

278
00:39:57.130 --> 00:40:01.690
当然 它是最有效 最方便实现的一种方式

279
00:40:10.050 --> 00:40:11.850
那我们通过强化学习的训练

280
00:40:12.450 --> 00:40:16.610
那么针对于 

281
00:40:18.910 --> 00:40:25.710
节约函数 我们也可以 通常我们也可以把它做softmax转换 是吧

282
00:40:26.260 --> 00:40:39.300
也也能让他求导是吧需要做金色比如说我们的节约函数是吧 是简单的画一下哈节约函数

283
00:40:41.010 --> 00:40:52.610
的形状的 那假设我们有坐标

284
00:41:02.840 --> 00:41:05.640
我们所看到的节约函数

285
00:41:06.230 --> 00:41:13.150
是形式哈形式

286
00:41:14.780 --> 00:41:24.220
样地 那呢 样地

287
00:41:38.450 --> 00:41:42.170
要么是零样 医生把节约函数是吧 我们可以用

288
00:41:42.940 --> 00:41:51.900
c贸易的函数来模拟他 我给他对吧整错了写写因为时候就得用t h了

289
00:41:57.730 --> 00:41:59.250
正常时间还是样

290
00:42:00.090 --> 00:42:09.330
形式样从画的有点丑形式

291
00:42:09.580 --> 00:42:16.100
是吧 那么我们可以在0 5位置

292
00:42:16.720 --> 00:42:22.920
来搞c末的函数 是吧 就可以模拟他了 你可以控制他的缩放

293
00:42:23.840 --> 00:42:31.400
收放还是可以算 只不过呢会用到一些近似的

294
00:42:32.610 --> 00:42:43.170
方式 或者是方法 比如说是形状

295
00:42:43.500 --> 00:42:51.500
对吧 形状是吧 你可以找c末尾的函数来金似的

296
00:42:53.630 --> 00:42:57.190
你可以做 当然 有的时候可以通过强化学习的训练

297
00:43:00.060 --> 00:43:03.940
额 那么其他的一些变种哈用

298
00:43:04.390 --> 00:43:11.310
局部注意力机制 当q只和输入序列当中的部分元素相关的时候

299
00:43:11.890 --> 00:43:18.450
 我们呢 就不需要再继续考虑 我们范围之外的

300
00:43:18.670 --> 00:43:30.830
 比如说我们在进行翻译的时候 或者是我们操作的时候我们的上下文 原始的情况下 那么上下文他比较范围比较大 还算比较大

301
00:43:31.840 --> 00:43:39.840
那么只是说我要通过滑动窗口的概念 我只选当前词的左边和右面时候考虑三个元素

302
00:43:41.380 --> 00:43:47.380
我只需要计算loco位置 loco位置我 我 我不需要考虑太多

303
00:43:48.090 --> 00:43:53.770
我只需要考虑三个就可以了 那么只关注部分位置上的信息

304
00:43:54.110 --> 00:44:00.190
时候叫做local的电视机制 与之对应的传统的方式叫做go

305
00:44:03.560 --> 00:44:07.520
 是另外的一种方式 也有样做

306
00:44:09.860 --> 00:44:17.340
 那么多头 穿梭门 穿梭门里面用到多头注意力机制

307
00:44:17.880 --> 00:44:26.400
多头注意机制的 也很简单 你看他词 他名字取得稍稍有1 析哈

308
00:44:27.880 --> 00:44:37.000
但是呢 实际上也不也也不难 对吧你会看到 对吧 真是还是我们刚才的狗屎qk个b是吧

309
00:44:37.780 --> 00:44:43.340
q k v 是吧 三个输入 那么输入的时候呢 你会发现箭头比较多 是吧

310
00:44:44.110 --> 00:44:49.630
那在里面叫做嗨嗨 那么他会有多个查询

311
00:44:50.680 --> 00:44:57.760
q一q二哈不同的查询 他想要计算 针对于不同的查询我们来看那么头

312
00:44:58.530 --> 00:45:07.010
不同的头最后得到的信息是不是也不一样 就相当于我们在cn里面用到的传统的概念 是吧 通道的概念

313
00:45:08.340 --> 00:45:13.500
之前我们说过cnn里面我们的每 和函数 对吧

314
00:45:14.790 --> 00:45:19.710
会对应颤抖对吧 大家大家应该还记得 那么

315
00:45:21.110 --> 00:45:29.110
实际上 我们的每个盒 它是从想要从数据里面去选不同特征 合合之间的和函数是不一样的 各种方式不一样

316
00:45:29.450 --> 00:45:35.370
一般我们会用50或者100个来做 那里面你可以选择

317
00:45:35.610 --> 00:45:41.290
你也可以选择 你可以选择q11直到km 你选择m个

318
00:45:42.300 --> 00:45:49.100
那么m个害的呢 就对于马腿的推伸 里面坑坑开  con concnate就

319
00:45:49.830 --> 00:46:03.070
就肯看到一起拼接到一起 是吧 拼接到一起 符号表示下面拼接 那么拼接到一起的时候k和v 他是相同的 你会发现 对于不同的藤山函数 k和v是相同的q 比

320
00:46:04.120 --> 00:46:17.720
 只是说想能做叠加 说呢 在里面 他有病型和操作 病型操作他会很快 那么病型操作呢 他可以要计算

321
00:46:19.000 --> 00:46:24.320
从输入信息当中选取多组信息用多组attentation

322
00:46:25.210 --> 00:46:33.410
来选取用多组头来选取attation 操作 呢 通过attentation再选取

323
00:46:33.860 --> 00:46:43.620
输入里面信息 通过k和v还是有对应关系的 从 从而进行v的组合

324
00:46:45.270 --> 00:46:51.670
话呢 每注意力 每注意力 其中或者每头 它只关注输入信息的

325
00:46:52.210 --> 00:47:00.810
某一类特点某一类特点那有同学说到哪一类特点的 那么时间网络确实稍微有点复杂哈

326
00:47:01.450 --> 00:47:09.530
有点复杂目前来讲哈 大家还是在探讨阶段 但多头注意力机制设置

327
00:47:10.620 --> 00:47:20.020
设置 那呢也是现在通用的做法 我们也就不同的头多头注意力机制 它呢是

328
00:47:20.720 --> 00:47:26.400
平行的结果 刚才看了 你看他是叠加的定型计算 效率比较高

329
00:47:27.350 --> 00:47:34.470
那么呢 还有一些人提出的 我可以用垂直的结构 随时就用hrugical的

330
00:47:34.970 --> 00:47:41.290
方式 他们认为呢 输入信息 它是有层次的 它不是平行的 有层次的

331
00:47:42.040 --> 00:47:47.720
 有人呢 也做多层的猫腿孩子的拼接 对吧 我们也很常见的

332
00:47:47.990 --> 00:48:00.630
 说呢 比如说我们的在语言里面 会有词 句子 段落 篇章等等 不同力度种层次 我们之前接触过

333
00:48:01.530 --> 00:48:07.290
hn模型大家还记得是吧 hn模型 他实际上在hn模型下

334
00:48:07.820 --> 00:48:16.340
的扩展h a n模型 对吧 大家还记得对吧 他对应的是

335
00:48:16.950 --> 00:48:22.830
而hererocky口

336
00:48:24.580 --> 00:48:32.580
attation有常有一种做法

337
00:48:37.770 --> 00:48:41.370
对吧 对应的 层次的信息 首先

338
00:48:42.070 --> 00:48:57.590
大家看图如果看不清的话我放大一点哈哎呀最开始

339
00:48:58.340 --> 00:49:07.740
如果我们k和v六一样的话 那再输入w哈 地方是经过了w 之后 他就会聚焦到

340
00:49:08.260 --> 00:49:14.780
三层次上 三层次 它有语义是吧 那么在对三层级压缩的话 它会到段落级别

341
00:49:15.230 --> 00:49:20.950
 多了级别的他根据不同的perry他会计算出不同的特性

342
00:49:21.470 --> 00:49:27.150
是些三层之间有组合的关系 呢叫做分层注意力

343
00:49:33.010 --> 00:49:33.130
345
00:49:42.360 --> 00:49:53.400
哈瑞亚都是由外部给出的不给出那么当查询来自于输入序列本身的时候就q来自于本身的时候

346
00:49:53.820 --> 00:50:04.780
 那么输入里面的每词儿都作为cry的时候 都作为cry的时候

347
00:50:05.250 --> 00:50:12.170
那么我们把它叫做自主的形式 自主的进程 他里面

348
00:50:12.850 --> 00:50:21.330
k q是x k也是x v也是x 但是呢

349
00:50:22.110 --> 00:50:27.910
我们是不是比如说在里面

350
00:50:30.400 --> 00:50:34.360
前后句前后句去去预测句话

351
00:50:34.950 --> 00:50:46.310
下面的句话下面那句话应该怎么样写 怎么样写 那么时候 一般来讲对应去对应生成的模型话呢

352
00:50:47.080 --> 00:50:52.640
qk v三项都是来自于输入的句子本身

353
00:50:53.380 --> 00:51:01.620
 都来自于的书句子本身 但是的 我们需要对他进行个变化 线性变化

354
00:51:02.350 --> 00:51:16.830
也用w 三个w 三个矩阵可以让他来学 那么在里面的x还是n的输入信息 通过线性变换等到不同 可以得到不同的查询向量大q 大k和大v

355
00:51:20.670 --> 00:51:23.310
那么 三个项链分别对应查询项链序列

356
00:51:23.800 --> 00:51:30.320
 简直建议下来水里和直下来水 我们再用电视公式

357
00:51:31.030 --> 00:51:38.190
引入思活方式 还是可以计算的 那么话呢 自助于机制 自注意力机制

358
00:51:38.640 --> 00:51:45.560
就可以把我们的 比如说是循环时间网络里面的一组输入来客服在

359
00:51:46.390 --> 00:51:55.030
使用的时候后面的信息 压缩到最后的时候 把前面的信息有些遗忘

360
00:51:56.160 --> 00:52:01.800
比如说种长句的一代机制 我也可以解决 可以让句子

361
00:52:02.180 --> 00:52:10.060
 对于每位置上的输入都有所考虑 呢 对应的自主self开始机制

362
00:52:11.010 --> 00:52:18.570
造法 但是呢 不只是q k v都相同 而且他们需要有变化

363
00:52:18.940 --> 00:52:30.980
 需要变化 当然你说老师你你你别不变换行不过不变了可以 他可可怕的模模型容量比较低而已 你没你没有经过变化的话 东西就不就没有学

364
00:52:31.680 --> 00:52:38.480
对吧 他就相当对角镇 对角镇 对角镇上的最小现象值都是一对吧

365
00:52:38.860 --> 00:52:47.180
现在效果 我们看一下注意力的应用 讲完注意力机制 看怎么用 哈 说了很多

366
00:52:48.110 --> 00:52:55.430
注意器是怎么用的 比如说 我们评论里面 刚才也举过例子了 会有情感

367
00:52:56.200 --> 00:53:05.960
情感的他会有注意力 注意力机制 比如说 有人写了么一句话

368
00:53:06.600 --> 00:53:12.480
那么 句话里面么长 到底哪些词对我们的情感分类起作用

369
00:53:13.590 --> 00:53:18.510
一些不撞词我就需要过滤掉了 对吧 比如说

370
00:53:19.550 --> 00:53:24.590
我们训练模型 完毕之后我们会发现deliciousamazing

371
00:53:25.230 --> 00:53:36.270
词对于情感分类任务特征非常重要 那么我们就可以 我们模型就会让注意力集中到区域

372
00:53:37.140 --> 00:53:44.500
 提升到区域对吧 另外 我们需要强调的是注意力机制他用到的参数

373
00:53:45.000 --> 00:53:54.400
不只是单独学习的 不是单独学习的 而且他还会去影响到你模型输入的一般性的调整上

374
00:53:56.790 --> 00:54:01.790
另外比如说 我们继续翻译当中的词和词 语言和目标语言对齐

375
00:54:02.380 --> 00:54:10.420
 比如说我在翻译spaceando神哈 那么他对应的对吧 我在我在生成的时候

376
00:54:10.870 --> 00:54:19.070
对吧 我一定要知道原始的词汇是 对吧 我一定要知道原始词汇是什

377
00:54:19.660 --> 00:54:25.460
那么话 肯定是原始的spaceando神对宫廷和海洋翻译的时候

378
00:54:26.260 --> 00:54:42.740
是最重要的 但是但是还有好处他还可以 比如说我翻译space的时候词的时候 他可以考虑到我上下文对他的影响 我可以在上下文里面去选择哪些词 还会对他有帮助

379
00:54:43.260 --> 00:54:50.540
 那么不光是过滤噪音 还可以把我们需要用的信息提取出来 让我们的翻译转换

380
00:54:50.950 --> 00:54:56.350
更准确比如说space space翻译成

381
00:54:58.800 --> 00:55:05.120
空间是吧 当然他会 他还会翻译成其他的东西是吧 空格是吧 空格是吧

382
00:55:05.570 --> 00:55:12.090
对吧 为在上下文下 他就需要翻译成空间 是吧 那么上下文的信息我focus到一些

383
00:55:12.600 --> 00:55:20.520
上下文 比如说领域的些重要的词汇 他也会提升我的效果 比如说图文转换

384
00:55:21.190 --> 00:55:28.630
多门转换里面对应的谁是多么态的任务 多么态 里面模态指的是文本模态 图片模态

385
00:55:29.210 --> 00:55:36.490
等等 模态 在多模态任务里面有非常常见的任务 那么

386
00:55:37.530 --> 00:55:42.930
叫做captioning 图文描述生成的

387
00:55:43.610 --> 00:55:59.810
过程 那比如说我输入amage amage 他可以通过一些卷辑网络或者是其他的图像 衣橱里的工作 把它卷基成不同的feature 不同的feature 那么feature

388
00:56:00.350 --> 00:56:06.750
我们如果要根据些feature来生成门门的话 那么它是

389
00:56:07.070 --> 00:56:12.790
哎 不是的 flying偶尔的八点我的 那么对于生成的过程当中

390
00:56:13.370 --> 00:56:20.290
他呢也需要用到助理机制 也是比如说我我我 我在做描述的时候 比如说

391
00:56:20.660 --> 00:56:26.420
abird 那么他是词和特征 特征 两个特征共同组成了

392
00:56:27.460 --> 00:56:43.140
 那么flying他有一些非的特征 对吧 那和他需要在进行翻译的过程当中 那不是说头等的对待图片 他会对每图片的切块有着重的考虑去影响

393
00:56:43.690 --> 00:56:51.170
雨衣的压缩 一级的时间好吧 那么是

394
00:56:52.570 --> 00:56:58.890
真实的一些一些相关的一些内容我想的内容以及它的使用

395
00:57:05.440 --> 00:57:07.320
好 我们先休息一下

396
00:57:08.070 --> 00:57:11.910
休息一下 休息五分钟 一会儿再回来

397
01:07:58.970 --> 01:07:59.090
好

398
01:08:01.020 --> 01:08:01.860
让他接着来

399
01:08:06.430 --> 01:08:12.710
下面我们来看一下transformer就说了呢 有了 

400
01:08:13.710 --> 01:08:20.110
三点左右时间了后面 无论是图像还是

401
01:08:22.290 --> 01:08:33.650
自然言还是那多么太还是声音 可以说是大一桶的种解决方式 那么刚才提到猫腿汉的天使

402
01:08:34.310 --> 01:08:40.590
里面的也是 transformer的重要的组成部分

403
01:08:45.120 --> 01:08:49.640
那么传双面 神经网络的结构 哈

404
01:08:50.610 --> 01:08:56.010
或者神经网络的架构 他那实际上基于attention

405
01:08:56.580 --> 01:09:05.780
几句白的 那么实际上他是起源于attentation的奥又腻篇论文m o p上发表了

406
01:09:06.020 --> 01:09:11.820
 那么 他也是提出了推身机制 还是基于种推身机制 在

407
01:09:12.890 --> 01:09:17.250
完全是要推出来实现来在在

408
01:09:19.610 --> 01:09:25.570
那或者二人上种传统的结构上都取得了非常好的

409
01:09:26.070 --> 01:09:32.270
效果 的是穿方面的来源 那么穿方面它的基本结构

410
01:09:33.050 --> 01:09:40.770
样 还有 隐扣的 有底扣的两部分 隐扣的是编码 底扣的是鞋码

411
01:09:41.780 --> 01:09:50.180
编码的部分的 我们可以看到   里面有multiha的吞噬呢 还有钱柜神经网络

412
01:09:50.540 --> 01:09:56.140
呢 解码的部分的同理 很类似 他的

413
01:09:56.700 --> 01:10:05.820
也是 有maskmulti的真是那还有multi的天神 他是两个拼接到一起的 只不过输入有所不同 输入有所

414
01:10:08.340 --> 01:10:13.140
那我来具体看一下 在图里面在图里面 

415
01:10:13.640 --> 01:10:19.920
蓝颜色的哈就表示迎扣的 浅粉色的是低扣的 其中

416
01:10:20.370 --> 01:10:26.650
标为橘黄色的三个部分用到的monty派的退税

417
01:10:27.560 --> 01:10:34.400
实际上 你会看到其他的部分都比较简单了

418
01:10:34.650 --> 01:10:40.170
对吧 钱柜神经网络是吧 加呀 取闹 卖z神等等 

419
01:10:41.500 --> 01:10:49.180
抵扣的部分 也有线性层 还接了sunflight分类器 是吧 他最后输出的是

420
01:10:49.660 --> 01:11:00.500
概率的值是吧 如果要分类问题的话 每类别的概率 那么引扣的里面 你扣的里面

421
01:11:01.560 --> 01:11:07.600
包含两部分 是multihead的self身套注意里面用的是

422
01:11:08.080 --> 01:11:14.280
自主的机制 咱刚才讲的 最后的那方法 那么第二部分呢

423
01:11:14.770 --> 01:11:26.770
潜规时间网络 那么底扣的部分呢 他用了两个multiheadtation 其中下面的是自主一定机制

424
01:11:27.300 --> 01:11:36.260
 那么第二个 第二个他不是自主意的机制了 是吧 你会看到他是用incoder的输出作为他的

425
01:11:36.830 --> 01:11:42.510
p和v六来宽蕊过来 前一层的宽蕊过来

426
01:11:42.890 --> 01:11:52.250
 那么是 他还他还会有前会时间网络是吧 最后输出输出

427
01:11:54.530 --> 01:12:03.850
输出的最后会有抵扣的哈 抵扣的 它可以输出每位置号或者整体的对应的词 它的出现的概率是

428
01:12:06.270 --> 01:12:11.510
在里面的还有重要的哈 你会发现的 实际上东西他他为

429
01:12:12.260 --> 01:12:22.020
如果地方仅仅是用病形的输入的话 他没有没有种前后的顺序结构 顺序结构 说在里面的

430
01:12:22.390 --> 01:12:27.910
一定要加入position的空点 就位置信息 实际上呢 我们

431
01:12:28.320 --> 01:12:38.520
 看的是如果模型哈不加位置信息的话 他可以被理解成白狗棍子模型 我们传统经常提到磁带模型

432
01:12:39.380 --> 01:12:52.580
自带模型 它没有位置 它磁带模型 是吧 那么但是加上位置之后每个词 它出现的每词 它对应的位置都有不及时应扣掉的话话

433
01:12:52.920 --> 01:12:58.920
词在不同的句子里面 它出现位置不同 位置不同的吧 它给人的编码 就不一样

434
01:12:59.070 --> 01:13:07.270
对吧 说位置编码在解决文本输入的时候很关键 也很重要 他会让你

435
01:13:07.810 --> 01:13:16.050
在模型能够捕捉到顺序的信息 说呢 也是我们在双门 一些一些

436
01:13:16.520 --> 01:13:22.840
额模型里面我需要注意的话一定要加入信息哈额

437
01:13:23.570 --> 01:13:29.650
我们看一下穿方面里面的天使图呢 有一点小 放大一下

438
01:13:32.820 --> 01:13:36.460
那么传说里面的天使刚才也也也提到过

439
01:13:37.830 --> 01:13:46.870
 那么多头注意力机制 我们可以看看到他 他相当于是输入的 是两层的 两层

440
01:13:47.680 --> 01:13:54.320
多头注意力机制在底扣的里面 他也他也有两层 两个层

441
01:13:55.870 --> 01:14:06.990
那么根据k v q 它的来源 k v六块的来源可以分为self tation刚才说过了以及呢 隐扣的抵扣的

442
01:14:07.990 --> 01:14:14.030
地方你看左边的一空的 里面用的都是self的腿神抵扣的 里面用到了隐扣的抵扣的

443
01:14:15.360 --> 01:14:34.560
但是在抵扣的里面最下面的一次是self退市 那么用着自注意力机机制 它的好处就可以简化每一层计算的复杂度 也可以 因为它是motthead的方式 它可以实现变形的

444
01:14:34.640 --> 01:14:44.600
计算 同时呢 当然了 隐扣的跟抵扣的没法办法变形哈 隐扣的跟抵扣的抵扣的必须得隐扣的计算完了之后才能抵扣的

445
01:14:45.830 --> 01:14:51.350
而且呢 通过自助以及机制可以解决网络当中长距离依赖的

446
01:14:52.300 --> 01:15:01.940
依赖的 那么在里面 你可以看到每雷尔 他的

447
01:15:02.910 --> 01:15:08.390
复杂度对 用到的计算 因为需要

448
01:15:09.110 --> 01:15:17.870
因为是attention嘛 你你有了有我吞噬的地方 你就必须要计算的参数 那么呢 是它的参数量

449
01:15:18.390 --> 01:15:35.230
穿出来那么穿双门他的代码我也推荐客因为都开源的哈我给推荐大家看一下 可以去理解 当然也不是特别复杂哈

450
01:15:35.650 --> 01:15:43.370
是特别复杂 但是你要想把每细节都了解清楚了 还是需要花一点时间的 那么大家可以对照着论文来看

451
01:15:43.710 --> 01:15:50.830
 特深的奥运篇论文 我觉得还是对提升整体的

452
01:15:51.560 --> 01:15:58.560
自然园处理的功力还是有帮助的 可以看一下代码那么transformer 它有哪些用途

453
01:15:59.890 --> 01:16:05.850
可以呢 作为上下文相关的编码器解码器来建模序列的数据

454
01:16:06.600 --> 01:16:13.000
也可以作为特征抽取器应用到其他的房价里面 比如说g p t里面用的

455
01:16:13.460 --> 01:16:20.780
单项的语言模型可以直接应用穿窗门的抵扣子那么birt里面

456
01:16:21.160 --> 01:16:26.720
用到的双向原模型实际上用到传送的一勾的部分

457
01:16:27.860 --> 01:16:36.820
根本部分 只不过他在输入里面 他调用了mask操作 或者是他接住了masklang的

458
01:16:39.840 --> 01:16:45.800
那么些的内容哈 就作为tension部分的

459
01:16:47.170 --> 01:16:53.610
进阶吧 说实际上的讲的呢

460
01:16:54.130 --> 01:17:03.970
些方法的还是比较基础的方法 那么后续的如果大家有兴趣的话可以自己去看一下代码或者下进行一些实现

461
01:17:04.570 --> 01:17:10.090
好吧 把部分的能够更加深入的理解

462
01:17:16.890 --> 01:17:20.090
好 我们来看一下下内容

463
01:18:20.990 --> 01:18:26.110
好 我把刚才的ppt发到群里哈 让大家就不用着急了

464
01:19:00.060 --> 01:19:01.260
朱丽和穿锁门

465
01:19:01.900 --> 01:19:02.180
确定

466
01:19:22.660 --> 01:19:27.260
刚才呀 我们看到了情感分类 情感分类的实际上

467
01:19:28.900 --> 01:19:39.140
 现在分类任务 我也是一小子

468
01:19:40.440 --> 01:19:46.320
简单的一类种话 就作为大家上手的话可以给他看一下

469
01:19:47.490 --> 01:19:54.530
情感分类任务我们先看一下万达系统吧 先分类 情感 分类任务还是比较简单的

470
01:19:57.200 --> 01:19:58.360
我看一下里

471
01:20:20.640 --> 01:20:21.680
我们来看一下情感

472
01:20:25.950 --> 01:20:32.630
里面指的是文本的哈带有文本的 实际上现在有很多做多模态的话 包括把图像里面的人脸让他的来

473
01:20:33.860 --> 01:20:46.380
结合到一起 那么只我 我们只先看一下最基本的内容 那么文本文本的情感分类 说是作为资源处理里面最基本的东西 包括很多的开源代码

474
01:20:47.230 --> 01:20:59.550
 很多的 相当于一些基础的教程 大家都是从我们其他分类开始的 我们既然已经学了么多的模型 大家可以去进行一些

475
01:21:00.370 --> 01:21:06.290
实际的样操作 实际操作 那么我来看一下情感分类

476
01:21:06.770 --> 01:21:11.330
一般来讲 我们认为 分类任务分为句子级别的下分类会计目标呢

477
01:21:13.230 --> 01:21:20.270
两类及目标的刚才我举的那例子哈那对对对foodsgoodtheservicsby

478
01:21:21.530 --> 01:21:30.730
 种种类型他句子里面涉及到多个方面的情感哈 那么呢 用于我们分析用户的真实意图

479
01:21:31.350 --> 01:21:48.950
但是还是有帮助的 大家有兴趣可以用刚才提到的把全省的机制来进行尝试哈那么文本的情感分析哈他对应的英文词哈 实际上我们有的时候说分类 有的时候说分析 实际上不太准确哈 他对应的应该是

480
01:21:50.010 --> 01:21:52.690
三配问题 banana

481
01:21:58.680 --> 01:22:05.640
任务哈 那么任务呢 在不同的 在元处理 领域里面都有比较长时间的

482
01:22:06.280 --> 01:22:15.440
 工作 他的实际上 他有几个方面 一种的方面是意见挖掘 叫呸 你妈

483
01:22:16.280 --> 01:22:30.600
 对词的叫跟你money意见挖掘 就发掘文本当中的意见 他不是分类倾向性分析 他是分类问题

484
01:22:30.930 --> 01:22:39.730
不明思议 大家配理解 简单说的 就最带有情感色彩的种主观性的文本进行分析 处理 归纳和推理的过程

485
01:22:40.080 --> 01:22:46.760
 那么文本的情感分类主要强调的是倾向性分析

486
01:22:47.300 --> 01:22:54.420
包括句子级别的情 情感分类 合计目标的情感分类因为我们说是比较简单的任务 我们先来

487
01:22:54.930 --> 01:23:04.370
看一下他的事情 操作稳稳的情感分析 他在里面实际上也会引人分类 他也会引入分类器的概念

488
01:23:05.800 --> 01:23:19.800
输入文本 他的输出的是情感的急性 时候呢 实际上我们在很多电商网站呢 包括些在线的购物的网站呢 等等 那么都可以

489
01:23:20.200 --> 01:23:27.360
分析出大家的情感来对吧 比如说我们在购物的时候需要一些 阅读一下大家怎么样评论

490
01:23:28.550 --> 01:23:37.750
那么时候呢 实现电商网 网站也用 些技术来分析 不同的产品 大家的种满意的成果

491
01:23:39.020 --> 01:23:44.420
 些例子的都比较简单哈 给文本 你去分析正向思想 中性

492
01:23:45.360 --> 01:23:51.920
 在里面有两点需要注意 是它不是二分类 它里面有中线

493
01:23:52.570 --> 01:24:02.050
因为确实是有好多的词或者是句子 他的确实没有表达正向和浮想的情感 仅仅是一些中性的情感

494
01:24:02.340 --> 01:24:10.300
对吧 那么为 那呢 可以通过

495
01:24:12.520 --> 01:24:17.200
文本分析是情感的 种文本的情感分析来了解情绪

496
01:24:17.830 --> 01:24:26.710
无聊生气呀好奇呀等等还有不同的情感情感的有的时候也会借用心理学的

497
01:24:27.250 --> 01:24:34.610
情绪分类分类方式比如说有生气有愤怒 一共在在标准的

498
01:24:35.230 --> 01:24:42.910
心理学 心理学里面一共有八个类别 有的时候也样分 那么都是不同的分法哈 进行多分类

499
01:24:43.770 --> 01:24:52.570
那么 要让计算机真正理解种自然 日常的交流 种词汇 单纯从单词方面的

500
01:24:53.110 --> 01:25:01.710
还不够的是吧 我们说能不能理解人的情感 那么呢 都是在不断探索的问题 标和电影评论

501
01:25:02.260 --> 01:25:08.140
包括一些股票的预测 是吧 看大众有没有恐慌的情绪

502
01:25:08.890 --> 01:25:20.690
在些领域还不会不会作为书籍额举最真实的例子哈呢是亚马逊网站上的例子哈从英文网站阿巴桑到com里面提出来

503
01:25:21.300 --> 01:25:27.860
里面的他包括了 你才会不同的人会写reviews 就不一定写评论

504
01:25:28.720 --> 01:25:38.320
在里面看到哈针对于惠普大衣 有377条评论 那么他给他打分的情况

505
01:25:39.090 --> 01:25:46.850
那么是整体打粉哈是四星 满分是五星对吧 那么针对于不同的评论来讲 他可以看到

506
01:25:47.440 --> 01:25:56.880
一星二星三星四星五星看分布的情况五星的是最多的 一星跟四星 脊椎其后

507
01:25:57.930 --> 01:26:12.930
那么 实际上 我们如果想再进一步细力度的分 比如说你说单机是否好用 他性价比怎么样 是不是配置起来很方便的 二小 卡里

508
01:26:13.600 --> 01:26:20.080
等等 你可以进行记录的分类 分数可以是人打出来的 也可以是模型生成

509
01:26:20.690 --> 01:26:27.530
对吧 那么话可以让商家也可以让用户更全面的了解大家关注的点

510
01:26:28.060 --> 01:26:40.660
对吧 比如说来了一句话 你可以首先进行分类 他到底是在说些评论的类别里面的哪一类 你可以逐句的分析 样可以让我们的分析更加的力度细化

511
01:26:43.680 --> 01:26:50.560
额 另外的也可以用于舆情 舆情也是些年来的一些热点 对吧分析

512
01:26:51.190 --> 01:27:02.470
网站上对不同的问题 他们的观点的是样的 那么句子级别句子级别的情感分类

513
01:27:03.050 --> 01:27:09.810
唉 也有很多很多 里面包括 输入 是句子

514
01:27:10.660 --> 01:27:16.180
 那么他的还有类别 他有m个类别 m可以是积极中心消极

515
01:27:17.000 --> 01:27:28.000
 呢 也可以是其他的 比如说来了输入 送菜速度快 服务好是作为味输入的模型 实验

516
01:27:28.380 --> 01:27:38.140
卡塞发音输出的是接机的周星的还是消极的训练样 那么经常用的句子的数据机chinesesantment

517
01:27:40.280 --> 01:27:51.120
是二分类的情感 包括我们的旅游出行 包括我们的书籍 包括我们的电子产品网购的评论 是大家已经标注好

518
01:27:52.560 --> 01:27:58.760
如果评测的指标是准确率的话 那么你需要构造模型 看他模型预测的怎么样

519
01:28:00.340 --> 01:28:12.060
是怎么样的 实际上呢 东西可以作为大家的最后的 如果你想做一些简单的问题 对吧 你可以搭建你自己的模型 你来把它实现

520
01:28:13.340 --> 01:28:19.740
 并不难 是吧 并不难 说呢 在里面 可以考虑作为

521
01:28:20.560 --> 01:28:27.040
最入门的耐心

522
01:28:33.060 --> 01:28:35.700
哇 大家可以去的是上网上去搜索

523
01:28:36.490 --> 01:28:47.210
一些  搜索一下数据集哈 那么数据集的也不大

524
01:28:47.680 --> 01:29:00.720
 那你说你要没有计算资源的话 他也不会说是跑不起来对吧 他的正力负利和实际上都是均等的 一会刚才说了二分类是吧 他呢 还有

525
01:29:01.520 --> 01:29:12.440
验证机哈也有台型赛测试机哈是8 1的关系哈关系

526
01:29:13.720 --> 01:29:24.440
而且是平均的问题 如果呢 你想要做近阶类的问题 你可以做呢 比如说如果数据不平衡的情况

527
01:29:27.700 --> 01:29:36.660
你做的怎么做 间接经常会遇到种问题 标注数据不都是

528
01:29:38.020 --> 01:29:43.380
 一半一半的 对吧 可以考虑种情况

529
01:29:44.580 --> 01:29:52.620
也可以考虑的下面一种情况比如说

530
01:29:56.080 --> 01:29:58.920
数据标注不充分

531
01:30:00.410 --> 01:30:05.930
现在人数据集里面对吧是

532
01:30:08.910 --> 01:30:12.190
四千八四千八比例

533
01:30:13.160 --> 01:30:23.200
我们如果要数据标注不充分的情况下 对吧 或者不平衡的情况下我们应该怎么做 应该用样的技术 里面的就可以包括一些

534
01:30:25.150 --> 01:30:31.030
唉 老哥们太深了 分管的方式 迁移的方式

535
01:30:31.480 --> 01:30:37.640
 呢 做任务的方式 联合学习的方式

536
01:30:40.290 --> 01:30:41.370
的

537
01:30:45.350 --> 01:30:48.070
还可以有一些半监督的

538
01:30:50.760 --> 01:30:56.840
去解决数据不平衡的数据标准不充分的问题 有很多做法

539
01:30:57.250 --> 01:31:06.610
对吧 那么比如说你的咱们大家都关心的 最后的期末的评分 对吧 你做入门级的report

540
01:31:07.760 --> 01:31:12.680
那你的分数肯定不会很高 那么你要想拿到比较高的分数

541
01:31:13.540 --> 01:31:23.300
那么你肯定需要针对于不平衡不充分 或者是呢 有一些错误标注 数据标注不准确

542
01:31:28.010 --> 01:31:31.130
对吧 种情况 也是经常会发生的

543
01:31:31.600 --> 01:31:45.120
三类情况应该怎么样解决 如果说入门级的炼程完成了 入门级都联系完成优秀

544
01:31:45.510 --> 01:31:53.670
那么你可以得到75分 如果些情况完成的话 那你的目标只是100分

545
01:31:55.820 --> 01:32:00.180
举个例子哈例子当然你可以不做问题 也不做吧

546
01:32:08.660 --> 01:32:12.020
那么呢 也是说题外话哈关于我们的

547
01:32:12.990 --> 01:32:19.870
 作业的问题 那么实际上的有中文的

548
01:32:20.520 --> 01:32:32.600
还有英文是吧 有中文的有英文 那么英文里面 那么你可以研究呢 你还是同样 同样 你可以做mdb的数据

549
01:32:33.560 --> 01:32:43.080
m d b的数据集里边我可以做哪些进阶类的实验的进阶类的实验

550
01:32:43.740 --> 01:32:49.460
比如说  大家也可以猜测到是吧 模型怎么样去适应跨语言的人

551
01:32:55.270 --> 01:32:56.030
不要模型

552
01:33:00.760 --> 01:33:01.880
不同的语言都可以用

553
01:33:03.650 --> 01:33:09.570
另外 如果只有英文的训练数据 那么英文训练出来的模型

554
01:33:10.260 --> 01:33:15.740
是否可以用于中文

555
01:33:22.730 --> 01:33:24.170
那么呢快一点

556
01:33:25.410 --> 01:33:36.770
些的都是好人 那goodnews是呢 我们现在还有了本事 之后哈继续穿窗门布的一套东西哈

557
01:33:37.170 --> 01:33:42.210
我们提出了m不是行 他对应的mt零狗

558
01:33:53.870 --> 01:33:58.790
在基础上 你怎么样进行种跨语言的模型的搭建和跨语言任务

559
01:33:59.390 --> 01:34:15.390
工作也是近几类的人 但是他们的评价方式都一样 那么imdb他的数据比较多一点 他有5万条数据却也跟tasty 确定赛跟他解散都是一样 都是

560
01:34:16.260 --> 01:34:23.100
25000 那么他也是两个类别的比较简单 尤其种任务比较简单

561
01:34:23.680 --> 01:34:41.000
说二分类是吧 我只能给大家75分 那么对于句子级的情情感分类对吧 那么我们之前也学到了很多的方面的基础的知识

562
01:34:41.500 --> 01:34:47.340
我们可以怎么做呢 首先 基于词表的分词法 分词

563
01:34:48.200 --> 01:34:57.280
对吧 你来了之后你处理中文你一定要分词 对吧一定要分词 当然了 你传统需要分词 你有了不了之后做做分词意义

564
01:34:57.790 --> 01:35:09.470
问题不大 因为都是预训练过的哈 那么可以用正向逆向最大匹配 对吧 种方法

565
01:35:10.650 --> 01:35:20.450
 你也可以要基于统计模型进行分词 比如说呢 也可以用有很多工具哈 比如说结巴分词了等等

566
01:35:21.220 --> 01:35:29.340
来 基于原模型进行分词 也可以基于序列标注进行分词 哎呦 sorry 我说的假的序列标注哈 今天还

567
01:35:30.010 --> 01:35:36.250
那时间不太够 那么序列标注呢 里面进行分词的时候呢 有

568
01:35:36.850 --> 01:35:44.210
 黑的马可mother了有肯定是那软的被子mother了 还有适度学习端到端的种

569
01:35:44.690 --> 01:35:50.330
分词的方法 分词工具哈最简单的就有

570
01:35:51.550 --> 01:35:56.670
结巴分词或者是哈布纳l tk分词工具

571
01:35:57.680 --> 01:36:03.080
那么在鸡巴分词工具里面的也提供了不种不同的匹配的模式

572
01:36:03.490 --> 01:36:14.610
 呢 也可以进行每词的词性的提取 它进行词性标注 实际上 我们在倍的猫的里面 我可以把每词的词性加进去哈

573
01:36:15.150 --> 01:36:22.790
加进去我们的也可以加入一些自定义的词典

574
01:36:23.580 --> 01:36:37.020
支持种多元也做语言呢 只是繁体的种分词在里面呢 我们可以设定分词的模式是全模式 话他可以把

575
01:36:37.730 --> 01:36:43.170
可以组成词的些 把短语都给你取出来 每上面有概率

576
01:36:45.900 --> 01:36:51.340
等等等等呢 第二步呢 可以用

577
01:36:52.350 --> 01:37:06.350
结巴分子工具进行一下词性标注比如说句子是闹 级别是闹 情感是闹 分类是个闹 是吧

578
01:37:06.650 --> 01:37:12.170
的话 他连接词那么

579
01:37:15.310 --> 01:37:19.550
当然了 你可以引入一些情感词典 对吧 些呢 在情感

580
01:37:20.600 --> 01:37:25.200
比如说切感词切感词典里面他可以

581
01:37:26.130 --> 01:37:33.850
也把你的情感词典引入进来 那么有了情感词典之后哇 实际上 你可以

582
01:37:34.610 --> 01:37:40.850
我们拍拍脑子就可以想想出一种方式 情感词出现了多少次我就给他

583
01:37:41.670 --> 01:37:48.590
唉 出现的正向情感出现多的他们就正向了 负向情感出现多负强的呗的 你可以用最简单的方式

584
01:37:49.450 --> 01:37:55.090
那分词去填一词之后直接调用几个词词典对于每不同的词

585
01:37:55.670 --> 01:38:02.350
你可以设定权重 最简单的第一颗负一自行神

586
01:38:04.550 --> 01:38:08.750
同时呢 也可以为情感词的修饰副词

587
01:38:09.990 --> 01:38:14.710
 作为他的程度 来设置他的权重

588
01:38:15.860 --> 01:38:28.900
最简单的做法 有情感词典 还有程度词的词典程度词 都是副词 给买文中的每词标记情感分数 不是

589
01:38:29.100 --> 01:38:37.060
比如说太好 好肯定是正向情感 但是他前面加了程度副词 不是

590
01:38:37.740 --> 01:38:44.780
好变成了反向了 说呢用情其实成都副词负一成

591
01:38:45.200 --> 01:38:51.920
正向情感一就得以复印 样可以综合每个词的得分 给出本本的情感体现

592
01:38:53.460 --> 01:39:00.100
那么时候呢 我们可以循环便利句子里面所有单词去看 是否是情感词典里面出现的些词

593
01:39:00.980 --> 01:39:11.700
 可以最终得到得分 情感分类最简单做法

594
01:39:12.000 --> 01:39:19.760
就简单做法 有些同学说是不是太简单了 确实是比较简单

595
01:39:22.410 --> 01:39:26.330
我们来看一下积极学习的方法

596
01:39:37.420 --> 01:39:38.060
几时以防

597
01:39:42.410 --> 01:39:44.810
那么 相对于机器学习的方法

598
01:39:46.010 --> 01:39:55.130
我们就可以看一下 里面有哪些机器学习的方法 刚才的太简单了 你传统机器学习方法

599
01:39:55.970 --> 01:40:03.010
那么 我们的首先 如果想要用机器学习方法 需要收集数据集并且提取特征

600
01:40:04.760 --> 01:40:09.640
特征的也可以用种特征筛选 特征压缩特征选择的方法

601
01:40:10.370 --> 01:40:19.730
来做 呢 把训练机输入到逻辑斯回归 s v m 支持销量机 或者是觉得器等分类器

602
01:40:20.280 --> 01:40:27.560
决策处 或者是些集成模型对吧 g b d t等等分离器里

603
01:40:28.370 --> 01:40:34.770
训练模型 通过测试级 那么首先呢 他会经历获取训练级提取特征

604
01:40:35.690 --> 01:40:43.570
构建分类器 模型评估过程 那么比如说还有有有有句子你可以呢 把它

605
01:40:44.550 --> 01:40:52.750
进行分词 分词之后呢 转换成磁带模型 在模型之后你就可以调用了 对吧 是我们最传统的套路

606
01:40:57.030 --> 01:41:00.990
在里面呢 可以大家可以尝试

607
01:41:01.310 --> 01:41:10.350
skl就拍上的包大家看不太清楚

608
01:41:14.360 --> 01:41:18.360
那在里面呢 sk乐里面有很多的

609
01:41:18.890 --> 01:41:24.290
是cheatsheet 大家可以看一下 那么包括分类算法

610
01:41:25.280 --> 01:41:31.560
都有了 包括回归算法包括降为算法 包括聚类算法

611
01:41:31.930 --> 01:41:38.290
都有了那么大家呢就可以是吧去用

612
01:41:38.790 --> 01:41:48.030
现有的方法来调用别人已经写好的程序 或者是开源的包

613
01:41:52.030 --> 01:41:55.190
那么在里面sk那主要他

614
01:41:56.170 --> 01:42:04.730
基本上 涵盖了很多种主流机学习方法 包括回归呀 包括分类 包括聚类较为等等

615
01:42:05.000 --> 01:42:11.000
 那么他也是开园的哈 继续拍照哈大家可以试一试

616
01:42:12.020 --> 01:42:17.580
那么积极学习的方法呢 他我们所谓的深度学习一下的套路

617
01:42:18.110 --> 01:42:28.990
那么主要的通过基于emady loc up查表操作 把他的词典里面词对应的响亮找到 呢通过神经网络

618
01:42:29.870 --> 01:42:35.790
来计算它的概率 那么再通过

619
01:42:36.610 --> 01:42:46.290
额 softmax等等 是吧 种方法额是计算最后的种分类的结果 我们也可以用一系列的种磁线量

620
01:42:47.260 --> 01:42:54.700
 不是种model作为申请网的输入 那么在深度学习的方面呢

621
01:42:55.810 --> 01:43:01.610
首先还是分词 分词了之后 取出每词所对应的

622
01:43:02.250 --> 01:43:08.170
 embody吧此销量可以是一训练好的

623
01:43:08.680 --> 01:43:17.520
带到你的模型里来 上面的你可以接不同的 birt了阿啦c啦些都可以的

624
01:43:18.060 --> 01:43:26.540
比如说 你可以用弯头发或者glup词项量 对吧都可以用对吧 说在里面列举一些可以用到的

625
01:43:26.950 --> 01:43:35.150
与训练的成交量把握的吧 可以用

626
01:43:40.820 --> 01:43:42.100
club响亮

627
01:43:42.940 --> 01:43:52.180
也可以用lm等等你也可以用本子一点不行都可以

628
01:43:52.940 --> 01:44:01.260
些的是都可以参考的  得到了些磁项量之后

629
01:44:02.530 --> 01:44:15.610
那么不就可以后面接不同的分类器哈那么在里面的需要提的哈有中文预训练好词销量 也有英文一下好词销量 大家可以从不同的网站上进行下载

630
01:44:16.630 --> 01:44:21.310
那么又回到了cn的模型m o p2014年的cn

631
01:44:22.610 --> 01:44:28.210
或者叫泰 cn分类的模型 那么

632
01:44:29.880 --> 01:44:34.360
卷集合是吧 是234是吧 呢

633
01:44:34.910 --> 01:44:41.510
有100个通道 将不同长度的卷剂输出变量 最后

634
01:44:42.020 --> 01:44:48.460
变成用maxf零变成一哈 项链变成一项链maxf零变成一

635
01:44:50.120 --> 01:44:56.440
 那么最后通过全连接层来输出 全连阶层加骚麦分类器在位置

636
01:44:57.520 --> 01:45:03.520
里面有个全连接 加少百个四克 m l p

637
01:45:07.370 --> 01:45:07.490
加

638
01:45:13.330 --> 01:45:22.730
一种结构 但实现功能实现功能

639
01:45:36.520 --> 01:45:40.280
那么还模型呢 大家有机会让可以做一些尝试

640
01:45:43.100 --> 01:45:47.780
也都比较简单 对吧 你也可以用双向的s t m的方法

641
01:45:48.670 --> 01:45:57.150
那么分词分完词之后呢 用额从左到右的i c m 再有从右到左的s

642
01:45:58.430 --> 01:46:05.510
呢 把两个对应位置的h 也输出的h销量

643
01:46:05.970 --> 01:46:25.090
进行拼接 拼接拼接完了之后 拼接成长的向量呢 把长的向量的作为输入输入到m o p加上f x 一卡全连接加上 最后通过sofax预测出来最终的结果

644
01:46:28.090 --> 01:46:33.730
那么实际上在百度提供的耳里耳里 模型里面

645
01:46:35.020 --> 01:46:40.740
 刚才之前我们也讲到1 02点零是吧 你也可以 用2 0模型

646
01:46:41.040 --> 01:46:48.680
直接来用用训练在里面包括的通用的语义表达能力也比较强

647
01:46:49.160 --> 01:46:55.400
 大家可以实验 包括也可以用拍照拍照的平台来进行种操作

648
01:46:57.340 --> 01:47:01.460
那么也如果 如果

649
01:47:03.500 --> 01:47:13.460
你你想要用穿缩码模型 或者是bert 或者二点类似的 那么你在进行我们情感分类的时候 都需要经过流程

650
01:47:14.000 --> 01:47:19.720
流程 分词呢 去路 靠词项量

651
01:47:20.300 --> 01:47:26.020
呢输入到模型里面来 呢压缩的时候每位置

652
01:47:27.200 --> 01:47:37.880
有输出 当然了 在分类的时候 贝尔特冒的或者尔奶特用的是手位置就cs位置cs位置呢 一般是用于接分类器

653
01:47:42.300 --> 01:47:47.300
的 只在做句子级别的分类的时候 输入的时候举手就加

654
01:47:48.510 --> 01:47:58.030
那么输出呢 就用cs位置的输出进行管理 我们认为呢 当你在输入的时候加cs的时候呢 已经涵盖了

655
01:47:58.250 --> 01:48:04.570
就能够再输出 位置 就包括了整个一输入句子上下文的语意

656
01:48:07.840 --> 01:48:12.240
那么里面是说到了 我们可以用波尔太的呀一些

657
01:48:12.870 --> 01:48:15.870
结构来错来来进行操作

658
01:48:33.310 --> 01:48:42.790
那么呢 机器学习部分 经济学习部分 如何用机器学习的模型来实现情感的分类

659
01:48:57.870 --> 01:48:58.950
些学习方法

660
01:49:03.250 --> 01:49:07.570
呢 实际上我们晴朗分类哈 还应该

661
01:49:08.470 --> 01:49:19.070
 是还应该有那么

662
01:49:21.430 --> 01:49:30.110
是基于目标的其他分类哈刚才我们说都是整个句子的 全局的其他分类 那么基于目标的其他分类任务

663
01:49:30.780 --> 01:49:39.180
是近些年来大家研究的热门的话题 那么

664
01:49:40.230 --> 01:49:45.430
他和我们刚才所提到的尺 与传统的分类任务类似

665
01:49:46.130 --> 01:49:55.530
在定义哈 包括你输入了s 你不需需要输出情感 在里面呢 他还包括了

666
01:49:57.320 --> 01:50:02.640
arspect有不同的定义 阿斯派的可以是句子当中出现的词

667
01:50:03.420 --> 01:50:11.380
言外之意呢 当然了 有很多不同的种setup 不同的设置 任务你可以是指定

668
01:50:11.900 --> 01:50:18.180
包括食物 包括服务等等 提前指定也可以

669
01:50:18.670 --> 01:50:28.710
当你来了句子 或者是文本的时候 多个句子组成的段落的时候 你从当中提取出

670
01:50:29.450 --> 01:50:39.970
种的种目标 那么最终的输出预测a的种情感的类别

671
01:50:40.490 --> 01:50:45.930
别的情感了一点就a呢就指的是你方面方面 a对应的是二四百

672
01:50:46.410 --> 01:50:54.930
哈算 那一对应的是阿斯

673
01:50:56.170 --> 01:51:04.410
呢 任务和原来的设置有一些不一样 有一些不一样 那么c呢 肯定是一些利益好的情感类别

674
01:51:04.730 --> 01:51:13.970
 那么外的进行输出 a呢 目标词或者uspect是词组成的 也是多个词组成的

675
01:51:14.270 --> 01:51:22.310
 你也可以是提前指定的也可以是跟我临时在句子里面发行有很多种不同的 那么东西

676
01:51:25.130 --> 01:51:31.050
实际上呢 有些同学说为不翻译成目标 而且翻译成方面

677
01:51:33.660 --> 01:51:45.060
月份不同人有不同的翻译吧 当然英文只有词 阿斯百分

678
01:51:46.100 --> 01:51:54.580
 那么在数据集里面 一般常用的rice14 还有1616

679
01:51:56.660 --> 01:52:00.140
还有三天3 1半3 1半的是 

680
01:52:02.260 --> 01:52:07.340
比赛哈 每年他都会提供发出一些数据几下让大家来进行

681
01:52:07.940 --> 01:52:12.900
complation竞赛性质的

682
01:52:13.930 --> 01:52:20.010
我都 那么常用的 对于种阿斯派克来 我的种分类的rst是14

683
01:52:20.550 --> 01:52:29.030
16 还有来烫数据集在数据集里面呢 它就包括正向情感 互相情感和中性的情感

684
01:52:29.350 --> 01:52:35.510
 那么情感的分析呀 我们用到的 里面的数据节都英文的

685
01:52:36.310 --> 01:52:46.510
riceround是对应餐厅来看电子设备实际上常用的还有你来抄你看你来抄你数据

686
01:52:48.060 --> 01:52:54.660
些书籍是在网上都公开了 大家都可以下载 那么他的评论测目标是艾克斯和准确率

687
01:52:55.290 --> 01:53:00.810
和好宏观的那么

688
01:53:01.740 --> 01:53:09.420
 我们就反正是评测指标 看一下数据及状态在

689
01:53:10.200 --> 01:53:15.400
 riceround14里面好像你会看到数据量比较小

690
01:53:16.130 --> 01:53:22.730
2000多个泡子只有600多个牛手 native只有800多那么来发不是14上

691
01:53:23.130 --> 01:53:30.970
弄少了 那么你看到数据集之后 实际上 你应该想

692
01:53:31.590 --> 01:53:39.950
想到任务哈我们怎么样做进阶的练习哈进阶的练习

693
01:53:44.930 --> 01:53:47.090
他实际上 是两个领域

694
01:53:47.930 --> 01:54:02.530
两个领域 那么里面的提到了是rust领域 他上面训练的模型怎么样 可以

695
01:54:03.330 --> 01:54:18.050
帮助帮助一来窗顶sir来泡

696
01:54:19.470 --> 01:54:26.110
有的人 对吧 飞起就样

697
01:54:26.960 --> 01:54:33.000
你可以解决跨领域 因为毕竟他都是正向情感和负向情感 那么跨领域指的是的

698
01:54:33.550 --> 01:54:40.150
riceround上有标注而不标注

699
01:54:41.850 --> 01:54:47.130
是吧 人真是快

700
01:54:49.390 --> 01:55:00.110
呢 还可以做多任务 多任务你说他们单个训练都可以 都可以做模型 他们两个可不可以合到一起训练

701
01:55:00.880 --> 01:55:06.520
那么合到一起训练的模型能不能比单个训练的模型得到的效果

702
01:55:07.230 --> 01:55:14.190
好还是会更差 对吧 说里面就有多任务 多任务指的是同时

703
01:55:15.070 --> 01:55:16.390
themottics

704
01:55:24.860 --> 01:55:28.780
对吧 些都是可以大家去思考的问题

705
01:55:29.370 --> 01:55:39.610
些呢 也都是比较有意思的问题 是大家的正在做的问题 样话你就可以 比如说时才2000多 9000多 对吧 他们可以互为弥补 互为补充

706
01:55:43.840 --> 01:55:47.360
那么同时比如说我们在

707
01:55:48.200 --> 01:56:07.600
神经网络里面 无论是二人呢还是森音呢还是倍儿的都需要用到position项链 那么position项链其实还挺重要的 那position项链它如果对于种目标级的种情况分类的任务或者阿斯百来我的情感分类

708
01:56:08.440 --> 01:56:13.560
来讲 那么pc时 他应该对应到对应到

709
01:56:14.020 --> 01:56:20.380
里面的具体的词 比如说 我现在要想分析句话的服务 那么

710
01:56:21.330 --> 01:56:27.490
所有的position应该是和服务词距离 也是相对位置

711
01:56:28.060 --> 01:56:34.580
现在位置是多少 说我们需要把相对位置 012进行编码

712
01:56:35.250 --> 01:56:42.450
一份进行变化 东西呢 位置变化 它也是向量的一种表示

713
01:56:42.850 --> 01:56:52.770
表示方式 那么讲一下 比如说我们想实现件事哈 有申请网络 也最基本最简单的模型

714
01:56:53.530 --> 01:57:03.410
首先分词 并且以目标词为中心 因为我们是叫阿斯派卡 以阿斯派克为中心 算出句子当中每个词的相对位置

715
01:57:04.700 --> 01:57:09.420
呢 我们将各个词的表征 每词呢 它都有表征

716
01:57:10.020 --> 01:57:17.140
就深蓝色的他每个词对应的标准 再把它浅蓝色对应的位置向量

717
01:57:17.740 --> 01:57:24.180
 下来进行拼接 把拼接成输入的

718
01:57:25.380 --> 01:57:32.220
 作为模型的输入 你可以接任意的模型 模型

719
01:57:33.290 --> 01:57:41.250
比如说有istm样把每位置对应的位置 它会输入 ism有个输出

720
01:57:41.640 --> 01:57:48.600
 有时里面双向的 每一位置都会得到销量 呢 我们再可以调用铺顶

721
01:57:49.550 --> 01:57:58.310
 把它进行压缩 话就变成了句子下量 一句子下来 句子下量寓意 得到之后 我们在

722
01:57:58.860 --> 01:58:04.700
  经过softs函数 那么

723
01:58:05.290 --> 01:58:17.290
最后会获得他的分类 对吧 当然 在

724
01:58:19.920 --> 01:58:24.280
还有是联合抽取 联合抽取呢 他指的我的

725
01:58:24.690 --> 01:58:33.090
目标词没有给定 那么你在里面记你position你就没法定 对吧 说 那首先我们要确定目标词

726
01:58:33.520 --> 01:58:41.720
 确定文白斯之后 那呢对应的是kwordkwordsinstraction 还有一类任务自然原处理里面叫做kwords

727
01:58:49.240 --> 01:58:51.120
里人那呢

728
01:58:51.630 --> 01:58:59.310
有任务可以把里面的aspect给继续出来 首先要进行工作

729
01:59:00.240 --> 01:59:06.720
再进行同样的一些后续的情况好 那么我们呢 说到了

730
01:59:07.480 --> 01:59:13.080
把情感之类最简单的任务哈给大家过一下大家呢已经有

